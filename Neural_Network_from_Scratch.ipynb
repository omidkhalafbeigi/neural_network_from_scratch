{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBsvel9TTPvn9I594bqyIo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omidkhalafbeigi/neural_network_from_scratch/blob/main/Neural_Network_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "from numpy.random import normal, rand \n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.datasets import load_iris  \n",
        "from sklearn.model_selection import train_test_split "
      ],
      "metadata": {
        "id": "Wgl-O4fflQg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_iris()\n",
        "X, y = dataset.data, dataset.target \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True)"
      ],
      "metadata": {
        "id": "NVY2Rzk4yt-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = float(input('Enter learning rate: '))\n",
        "hidden_num = int(input('Enter number of hidden layers: '))\n",
        "hidden_nodes = list()\n",
        "activations = list()\n",
        "\n",
        "for h in range(hidden_num):\n",
        "  nodes_num = int(input(f'Enter number of nodes in hidden layer {h + 1}: '))\n",
        "  hidden_nodes.append(nodes_num)\n",
        "  hidden_act = input(f'Enter activation for hidden layer {h + 1} (sigmoid, tanh, relu, linear): ')\n",
        "  activations.append(hidden_act)\n",
        "\n",
        "print('--------------------------------')\n",
        "output_nodes_num = int(input('Enter number of nodes in output layer: '))\n",
        "hidden_nodes.append(output_nodes_num)\n",
        "output_activation = input('Enter activation output layer (sigmoid, linear): ')\n",
        "activations.append(output_activation)"
      ],
      "metadata": {
        "id": "sAeNFUDICe9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "167a56b5-8935-4358-c16e-6b9b52f302c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter learning rate: 0.2\n",
            "Enter number of hidden layers: 1\n",
            "Enter number of nodes in hidden layer 1: 64\n",
            "Enter activation for hidden layer 1 (sigmoid, tanh, relu, linear): sigmoid\n",
            "--------------------------------\n",
            "Enter number of nodes in output layer: 3\n",
            "Enter activation output layer (sigmoid, linear): sigmoid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_der(x):\n",
        "  return x * (1 - x) # Input should be sigmoid\n",
        "\n",
        "def tanh(x):\n",
        "  return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "\n",
        "def tanh_der(x):\n",
        "  return 1 - np.power(x, 2) # Input should be tanh\n",
        "\n",
        "def relu_apply(x):\n",
        "  if x > 0: return x\n",
        "  else: return 0\n",
        "\n",
        "def relu(x):\n",
        "  return np.vectorize(relu_apply)(x)\n",
        "\n",
        "def relu_der_apply(x):\n",
        "  if x > 0: return 1\n",
        "  else: return 0\n",
        "\n",
        "def relu_der(x):\n",
        "  return np.vectorize(relu_der_apply)(x)\n",
        "\n",
        "def linear(x): return x \n",
        "def linear_der(x): return (x / x)\n",
        "\n",
        "def mse(d, y):\n",
        "  return (d - y)\n",
        "\n",
        "def get_activation(function_name):\n",
        "  activations = {'sigmoid':sigmoid, 'tanh':tanh, 'relu':relu, 'linear':linear}\n",
        "  return activations[function_name]\n",
        "\n",
        "def get_activation_der(function_name):\n",
        "  activations = {'sigmoid':sigmoid_der, 'tanh':tanh_der, 'relu':relu_der, 'linear':linear_der}\n",
        "  return activations[function_name]"
      ],
      "metadata": {
        "id": "eYqjlV4flYIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weight_bias_init(input_dim, hidden_num):\n",
        "  weights_list = list()\n",
        "  bias_list = list()\n",
        "\n",
        "  if hidden_num > 0:\n",
        "    for h_idx in range(hidden_num):\n",
        "      if h_idx == 0:\n",
        "        w = normal(loc=0, scale=1, size=(input_dim, hidden_nodes[h_idx]))\n",
        "        b = normal(loc=0, scale=1, size=hidden_nodes[h_idx])\n",
        "      else:\n",
        "        w = normal(loc=0, scale=1, size=(hidden_nodes[h_idx - 1], hidden_nodes[h_idx]))\n",
        "        b = normal(loc=0, scale=1, size=hidden_nodes[h_idx])\n",
        "      weights_list.append(w)\n",
        "      bias_list.append(b)\n",
        "\n",
        "    w = normal(loc=0, scale=1, size=(hidden_nodes[-2], hidden_nodes[-1]))\n",
        "    b = normal(loc=0, scale=1, size=hidden_nodes[-1])\n",
        "    weights_list.append(w)\n",
        "    bias_list.append(b)\n",
        "  else:\n",
        "    w = normal(loc=0, scale=1, size=(input_dim, output_nodes_num))\n",
        "    b = normal(loc=0, scale=1, size=output_nodes_num)\n",
        "    weights_list.append(w)\n",
        "    bias_list.append(b)\n",
        "\n",
        "  return weights_list, bias_list"
      ],
      "metadata": {
        "id": "H6fTU8Y0lfL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X_train, w, b, hidden_num):\n",
        "  if hidden_num > 0:\n",
        "    pred, _ = forward_mlp(X_train, w, b)\n",
        "    pred = np.round(pred)\n",
        "  else:\n",
        "    activation = get_activation(activations[0])\n",
        "    pred = activation(np.dot(X_train, w) + b)\n",
        "    pred = np.round(pred)\n",
        "  return pred"
      ],
      "metadata": {
        "id": "qktOU-zxlhgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_mlp(X, weights_list, bias_list):\n",
        "  outputs_list = list()\n",
        "  output = X.copy()\n",
        "  for layer_idx in range(len(weights_list)):\n",
        "    w = weights_list[layer_idx]\n",
        "    b = bias_list[layer_idx]\n",
        "    activation = get_activation(activations[layer_idx])\n",
        "    output = np.dot(output, w)\n",
        "    output += b\n",
        "    output = activation(output)\n",
        "    outputs_list.append(output)\n",
        "  return output, outputs_list"
      ],
      "metadata": {
        "id": "5nxUOwpjljke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_mlp(X_train, y_train, hidden_num, epochs=10):\n",
        "  weights_list, bias_list = weight_bias_init(X_train.shape[-1], hidden_num)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    epoch_error = list()\n",
        "    for sample_idx in range(len(X_train)):\n",
        "      new_weights_list = list()\n",
        "      new_bias_list = list()\n",
        "\n",
        "      sample = X_train[sample_idx]\n",
        "      label = y_train[sample_idx]\n",
        "      output, outputs_list = forward_mlp(sample, weights_list, bias_list)\n",
        "      error = mse(label, output)\n",
        "      epoch_error.append(np.abs(error))\n",
        "\n",
        "      for layer_idx in range(hidden_num, 0, -1):\n",
        "        new_w = weights_list[layer_idx].copy()\n",
        "        new_b = bias_list[layer_idx].copy()\n",
        "        activation_der = get_activation_der(activations[layer_idx])\n",
        "\n",
        "        if layer_idx == hidden_num: gradient = activation_der(output) * error # For output layer \n",
        "        else: gradient = activation_der(outputs_list[layer_idx]) * np.dot(weights_list[layer_idx + 1], gradient.T)\n",
        "\n",
        "        # Update weight \n",
        "        for w_i_idx in range(new_w.shape[0]):\n",
        "          for w_j_idx in range(new_w.shape[1]):\n",
        "            y = outputs_list[layer_idx - 1][w_i_idx] # if layer_idx = 1, then (y) will be first hidden layer's output \n",
        "            grad = gradient[w_j_idx]\n",
        "            new_w[w_i_idx][w_j_idx] += (learning_rate * grad * y)\n",
        "        \n",
        "        # Update bias \n",
        "        for w_j_idx in range(new_w.shape[1]):\n",
        "          grad = gradient[w_j_idx]\n",
        "          new_b[w_j_idx] += (learning_rate * grad)\n",
        "\n",
        "        new_weights_list.append(new_w)\n",
        "        new_bias_list.append(new_b)\n",
        "\n",
        "\n",
        "      # For first layer parameters \n",
        "      layer_idx = 0\n",
        "      gradient = activation_der(outputs_list[layer_idx]) * np.dot(weights_list[layer_idx + 1], gradient.T)\n",
        "      new_w = weights_list[layer_idx].copy()\n",
        "      new_b = bias_list[layer_idx].copy()\n",
        "      # Update weight \n",
        "      for w_i_idx in range(new_w.shape[0]):\n",
        "        for w_j_idx in range(new_w.shape[1]):\n",
        "          y = sample[w_i_idx]\n",
        "          grad = gradient[w_j_idx]\n",
        "          new_w[w_i_idx][w_j_idx] += (learning_rate * grad * y)\n",
        "\n",
        "      # Update bias \n",
        "      for w_j_idx in range(new_w.shape[1]):\n",
        "        grad = gradient[w_j_idx]\n",
        "        new_b[w_j_idx] += (learning_rate * grad)\n",
        "\n",
        "      new_weights_list.append(new_w)\n",
        "      new_bias_list.append(new_b)\n",
        "      new_weights_list.reverse()\n",
        "      new_bias_list.reverse()\n",
        "      weights_list = new_weights_list \n",
        "      bias_list = new_bias_list\n",
        "\n",
        "    print(f'Epoch: {epoch + 1} - Loss: {np.mean(epoch_error)}')\n",
        "\n",
        "  return weights_list, bias_list"
      ],
      "metadata": {
        "id": "BxI-9Ly2lmkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_perceptron(X_train, y_train, epochs): # There is no hidden layer \n",
        "  weights_list, bias_list = weight_bias_init(X_train.shape[-1], hidden_num=0)\n",
        "  new_w = weights_list[0].copy()\n",
        "  new_b = bias_list[0].copy()\n",
        "  activation = get_activation(activations[0])\n",
        "  activation_der = get_activation_der(activations[0])\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    epoch_error = list()\n",
        "    for sample_idx in range(len(X_train)):\n",
        "      sample = X_train[sample_idx]\n",
        "      label = y_train[sample_idx]\n",
        "      output = np.dot(sample, new_w) + new_b \n",
        "      output = activation(output)\n",
        "      error = mse(label, output)\n",
        "      gradient = activation_der(output) * error \n",
        "\n",
        "      for w_i_idx in range(new_w.shape[0]):\n",
        "        for w_j_idx in range(new_w.shape[1]):\n",
        "          grad = gradient[w_j_idx]\n",
        "          y = sample[w_i_idx]\n",
        "          new_w[w_i_idx][w_j_idx] += (learning_rate * grad * y)\n",
        "\n",
        "      for w_j_idx in range(new_w.shape[1]):\n",
        "        grad = gradient[w_j_idx]\n",
        "        y = sample[w_j_idx]\n",
        "        new_b += (learning_rate * grad)\n",
        "\n",
        "      epoch_error.append(np.abs(error))\n",
        "\n",
        "    print(f'Epoch: {epoch + 1} - Loss: {np.mean(epoch_error)}')\n",
        "\n",
        "  return new_w, new_b"
      ],
      "metadata": {
        "id": "_9H7DNxwlo5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_mlp = list()\n",
        "for y in y_train:\n",
        "  if int(y) == 0: y_train_mlp.append([1, 0, 0])\n",
        "  elif int(y) == 1: y_train_mlp.append([0, 1, 0])\n",
        "  elif int(y) == 2: y_train_mlp.append([0, 0, 1])\n",
        "\n",
        "y_test_mlp = list()\n",
        "for y in y_test:\n",
        "  if int(y) == 0: y_test_mlp.append([1, 0, 0])\n",
        "  elif int(y) == 1: y_test_mlp.append([0, 1, 0])\n",
        "  elif int(y) == 2: y_test_mlp.append([0, 0, 1])"
      ],
      "metadata": {
        "id": "dCixkgQSzx46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100 \n",
        "\n",
        "if hidden_num > 0: w, b = train_mlp(X_train, y_train_mlp, hidden_num, epochs)\n",
        "else: w, b = train_perceptron(X_train, y_train_mlp, epochs)"
      ],
      "metadata": {
        "id": "IaGn5KdEltJy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "187540c1-d77b-41a6-976d-6b9669f784fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 - Loss: 0.32991604575549516\n",
            "Epoch: 2 - Loss: 0.2907834505198954\n",
            "Epoch: 3 - Loss: 0.27212548018658256\n",
            "Epoch: 4 - Loss: 0.2604081885043095\n",
            "Epoch: 5 - Loss: 0.25060481303968185\n",
            "Epoch: 6 - Loss: 0.24249673056975393\n",
            "Epoch: 7 - Loss: 0.2358501614478481\n",
            "Epoch: 8 - Loss: 0.23036708654420185\n",
            "Epoch: 9 - Loss: 0.2257275049846474\n",
            "Epoch: 10 - Loss: 0.22167503313162695\n",
            "Epoch: 11 - Loss: 0.2180602751606187\n",
            "Epoch: 12 - Loss: 0.21482874039448552\n",
            "Epoch: 13 - Loss: 0.21196907313786312\n",
            "Epoch: 14 - Loss: 0.2096314817903886\n",
            "Epoch: 15 - Loss: 0.15515380500192952\n",
            "Epoch: 16 - Loss: 0.11263440160815252\n",
            "Epoch: 17 - Loss: 0.10696739262295961\n",
            "Epoch: 18 - Loss: 0.10320548959757089\n",
            "Epoch: 19 - Loss: 0.10020534645551045\n",
            "Epoch: 20 - Loss: 0.09762347308438374\n",
            "Epoch: 21 - Loss: 0.09531746178966606\n",
            "Epoch: 22 - Loss: 0.09321392615387383\n",
            "Epoch: 23 - Loss: 0.0912726198783355\n",
            "Epoch: 24 - Loss: 0.08947082345919925\n",
            "Epoch: 25 - Loss: 0.08779371850584018\n",
            "Epoch: 26 - Loss: 0.08622746893879035\n",
            "Epoch: 27 - Loss: 0.08475467071084825\n",
            "Epoch: 28 - Loss: 0.08335460033203525\n",
            "Epoch: 29 - Loss: 0.08200730537329247\n",
            "Epoch: 30 - Loss: 0.08069567967264578\n",
            "Epoch: 31 - Loss: 0.07941086403758152\n",
            "Epoch: 32 - Loss: 0.0781552381819129\n",
            "Epoch: 33 - Loss: 0.07692177059241792\n",
            "Epoch: 34 - Loss: 0.0756884573374015\n",
            "Epoch: 35 - Loss: 0.07444521684860525\n",
            "Epoch: 36 - Loss: 0.07320307216303014\n",
            "Epoch: 37 - Loss: 0.07197782716074833\n",
            "Epoch: 38 - Loss: 0.07077558268877723\n",
            "Epoch: 39 - Loss: 0.06959522114058905\n",
            "Epoch: 40 - Loss: 0.06844594455049235\n",
            "Epoch: 41 - Loss: 0.06734987764044381\n",
            "Epoch: 42 - Loss: 0.06631855359641978\n",
            "Epoch: 43 - Loss: 0.06534647009182809\n",
            "Epoch: 44 - Loss: 0.06442444723071176\n",
            "Epoch: 45 - Loss: 0.06354678034561193\n",
            "Epoch: 46 - Loss: 0.06271133486332814\n",
            "Epoch: 47 - Loss: 0.061918503828950366\n",
            "Epoch: 48 - Loss: 0.06117015432746661\n",
            "Epoch: 49 - Loss: 0.060468195419013084\n",
            "Epoch: 50 - Loss: 0.05981294411036502\n",
            "Epoch: 51 - Loss: 0.05920189760380314\n",
            "Epoch: 52 - Loss: 0.05862900639079754\n",
            "Epoch: 53 - Loss: 0.058084202757567475\n",
            "Epoch: 54 - Loss: 0.057553254580369674\n",
            "Epoch: 55 - Loss: 0.057018018639670744\n",
            "Epoch: 56 - Loss: 0.0564571112650813\n",
            "Epoch: 57 - Loss: 0.05584682310235878\n",
            "Epoch: 58 - Loss: 0.05516061092805741\n",
            "Epoch: 59 - Loss: 0.05436695149394924\n",
            "Epoch: 60 - Loss: 0.053433036375436206\n",
            "Epoch: 61 - Loss: 0.052362508493667914\n",
            "Epoch: 62 - Loss: 0.051308277304797804\n",
            "Epoch: 63 - Loss: 0.05046794725487587\n",
            "Epoch: 64 - Loss: 0.04983837096709086\n",
            "Epoch: 65 - Loss: 0.04933877865068317\n",
            "Epoch: 66 - Loss: 0.048907075023409026\n",
            "Epoch: 67 - Loss: 0.04851597153809902\n",
            "Epoch: 68 - Loss: 0.0481563716471004\n",
            "Epoch: 69 - Loss: 0.04782260590512607\n",
            "Epoch: 70 - Loss: 0.04750946626261097\n",
            "Epoch: 71 - Loss: 0.04721264204656982\n",
            "Epoch: 72 - Loss: 0.046928973244864836\n",
            "Epoch: 73 - Loss: 0.04665632316679838\n",
            "Epoch: 74 - Loss: 0.04639332719301713\n",
            "Epoch: 75 - Loss: 0.046139156265195054\n",
            "Epoch: 76 - Loss: 0.04589333233340443\n",
            "Epoch: 77 - Loss: 0.045655593964105554\n",
            "Epoch: 78 - Loss: 0.04542580221919482\n",
            "Epoch: 79 - Loss: 0.04520387755810769\n",
            "Epoch: 80 - Loss: 0.04498976001087379\n",
            "Epoch: 81 - Loss: 0.04478338597079967\n",
            "Epoch: 82 - Loss: 0.044584675986946\n",
            "Epoch: 83 - Loss: 0.04439352910999808\n",
            "Epoch: 84 - Loss: 0.04420982059814923\n",
            "Epoch: 85 - Loss: 0.044033400968991894\n",
            "Epoch: 86 - Loss: 0.04386409536832748\n",
            "Epoch: 87 - Loss: 0.04370170295735504\n",
            "Epoch: 88 - Loss: 0.043545996482451665\n",
            "Epoch: 89 - Loss: 0.04339672240184621\n",
            "Epoch: 90 - Loss: 0.04325360193652086\n",
            "Epoch: 91 - Loss: 0.04311633324379628\n",
            "Epoch: 92 - Loss: 0.0429845946549811\n",
            "Epoch: 93 - Loss: 0.04285804865849285\n",
            "Epoch: 94 - Loss: 0.04273634612956919\n",
            "Epoch: 95 - Loss: 0.04261913026814428\n",
            "Epoch: 96 - Loss: 0.04250603982998006\n",
            "Epoch: 97 - Loss: 0.04239671149709266\n",
            "Epoch: 98 - Loss: 0.042290781562155606\n",
            "Epoch: 99 - Loss: 0.04218788740168221\n",
            "Epoch: 100 - Loss: 0.042087669389717704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = predict(X_test, w, b, hidden_num)\n",
        "nn_accuracy = accuracy_score(y_test_mlp, pred)\n",
        "print(f'NN Accuracy: {nn_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctlOUPJB4auc",
        "outputId": "6091aa65-ea62-41fa-e4fc-b4f44d715972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN Accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}